#include <types.h>
#include <kern/errno.h>
#include <lib.h>
#include <thread.h>
#include <curthread.h>
#include <addrspace.h>
#include <vm.h>
#include <machine/spl.h>
#include <machine/tlb.h>
<<<<<<< .mine
#include <uio.h>
#include <vnode.h>

=======
#include <elf.h>
#include <uio.h>
#include <kern/stat.h>
#include <vfs.h>
#include <kern/unistd.h>
#include <synch.h>
#include <vnode.h>
>>>>>>> .r24
/*
 * Dumb MIPS-only "VM system" that is intended to only be just barely
 * enough to struggle off the ground. You should replace all of this
 * code while doing the VM assignment. In fact, starting in that
 * assignment, this file is not included in your kernel!
 */

static struct {
<<<<<<< .mine
  u_int32_t  numofpage;
  u_int32_t  FirstPageAdd;
  struct page* firstPage;
}coremap;
=======
    u_int32_t  numofpage;
    u_int32_t  FirstPageAdd;
    u_int32_t  numofpageleft;
    struct page* firstPage;
    }coremap;
>>>>>>> .r24

//swap definition start
char* bitmap;
static struct bitmap *bit_map = NULL;
struct vnode* swap; 
u_int32_t swap_end;
u_int32_t swap_start;
u_int32_t swap_size;
u_int32_t swap_free;
struct cv* cv_needmore;
struct cv* cv_needevict;
struct lock* swap_lock;
//swap definition end

/* under dumbvm, always have 48k of user stack */
//#define DUMBVM_STACKPAGES    1

void
vm_bootstrap(void)
{
<<<<<<< .mine
  u_int32_t firstadd , lastadd;
  u_int32_t firstpage, lastpage , totalpage , mapsize;
  u_int32_t mappage;
  ram_getsize(&firstadd, &lastadd); 
=======
//************coremap start
u_int32_t firstadd , lastadd;
u_int32_t firstpage, lastpage , totalpage , mapsize;
u_int32_t mappage,bitmappage,bitmap_size;
ram_getsize(&firstadd, &lastadd); 
>>>>>>> .r24

  firstpage=(firstadd)&0xfffff000; //mask out addr
  lastpage=(lastadd)&0xfffff000;
  totalpage=(lastpage>>12)-(firstpage>>12);
  mapsize=totalpage*sizeof(struct page); // how big is the map?
  mappage=mapsize/4096+1; //how many pages are needed to store coremap

  coremap.FirstPageAdd=firstpage;
  coremap.numofpage=totalpage;

  // pages should be a kernel vaddr
  coremap.firstPage=(struct page*)PADDR_TO_KVADDR(firstadd); 
  bzero(coremap.firstPage, coremap.numofpage * sizeof(struct page)); //initialize everything to zero.. nessisary????
  coremap.FirstPageAdd=coremap.FirstPageAdd+(mappage<<12); //update content of coremap
  coremap.numofpage=coremap.numofpage-mappage;
  coremap.firstPage = coremap.firstPage+mappage;

<<<<<<< .mine
  int i;
  struct page* start=coremap.firstPage;
  for(i=0;i<totalpage; i++) {
	  start->allocated=0;
    start++;
  }
=======
coremap.firstPage= (struct page*)PADDR_TO_KVADDR(firstadd); 
//bzero(coremap.firstPage, coremap.numofpage * sizeof(struct page)); //initialize everything to zero.. nessisary????
coremap.FirstPageAdd=coremap.FirstPageAdd+(mappage<<12); //update content of coremap
coremap.numofpage=coremap.numofpage-mappage;
coremap.firstPage = coremap.firstPage+mappage;
//*************coremap end

//*************swap
struct stat disk;
char swapdevice[]= {"lhd0raw:"};
int err = vfs_open(swapdevice, O_RDWR, &swap);
	if (err) {
    		panic("can't open swap device\n");
	}

VOP_STAT(swap, &disk);
swap_size=disk.st_size/4096;
bitmap_size = (((disk.st_size)/4096)*sizeof(char));
bitmappage=bitmap_size/4096+1;

bitmap=(char*)PADDR_TO_KVADDR(coremap.FirstPageAdd);
int i;
for(i=0;i<bitmap_size;i++)
	bitmap[i]=0;

swap_start=totalpage/5;
swap_end=totalpage/2;

coremap.FirstPageAdd=coremap.FirstPageAdd+(bitmappage<<12); //update content of coremap
coremap.numofpage=coremap.numofpage-bitmappage;
coremap.firstPage = coremap.firstPage+bitmappage;
coremap.numofpageleft=totalpage;

kprintf(" i have %d pages\n",coremap.numofpage);
//*****************swap
//bit_map = bitmap_create(swap_size);
//lock cv create
swap_lock=lock_create("swaplock");
cv_needmore=cv_create("needmore");
cv_needevict=cv_create("needevict");

err = thread_fork("swapper", NULL, 0, swapper, NULL);
    if (err) {
        panic("Can't create thread for swap.");
    }
>>>>>>> .r24
}

static
paddr_t
getppages(unsigned long npages)
{
	paddr_t addr;
	
	
	if(coremap.firstPage==NULL)
	{
	addr = ram_stealmem(npages);
	return addr;
	}
	else 
	{
		//kprintf("trying to get %d page\n",npages);
		struct page* start=coremap.firstPage;
		struct page* freepagestart=coremap.firstPage + coremap.numofpage; //track where the first free page start, i set it to random value at first		
		//int found =0;
		
		int count=0;
		while (start<(coremap.firstPage + coremap.numofpage))
		{
				int i=0;
			while(start<(coremap.firstPage + coremap.numofpage) && !(start->allocated==0))
				{
				start++;   //skip all allocated pages
				i++;
				//kprintf("not free %d\n",i);
				}
				freepagestart=start; //save the begining of free page
			while (start<(coremap.firstPage + coremap.numofpage) && (start->allocated==0)) //check if there are enough consecutive avaliable pages
				{
				count++;
				start++;
				}
			if (count>=npages)
				{	
				addr=(paddr_t)(coremap.FirstPageAdd + ((freepagestart) - coremap.firstPage) * 4096);
				freepagestart->consec = npages;
				
				for(count=0; count<npages;count++)
					{
        				freepagestart[count].allocated = 1;
        				freepagestart[count].as = NULL;
					freepagestart[count].kernel = 1;
					}	
				coremap.numofpageleft=coremap.numofpageleft-npages;
				return addr;
				} 
		
		}

	}
	
	kprintf(" i failed to allocated %d nigga\n",npages);
	return 0;// resturn 0 if no page found
}

/* Allocate/free some kernel-space virtual pages */
vaddr_t 
alloc_kpages(int npages)
{
	//npages=1;
	int spl;
	paddr_t pa,pa2;
	spl = splhigh();
	pa = getppages(npages);
	if (pa==0) {
		return (paddr_t)0;
	}

	
	splx(spl);

	return PADDR_TO_KVADDR(pa);
}

void 
free_kpages(vaddr_t addr)
{
	int spl;
	spl = splhigh();
	paddr_t paddr;
	paddr=addr-MIPS_KSEG0;	
	
	struct page* currentpage;
	currentpage =(coremap.firstPage + (((u_int32_t)(paddr) - coremap.FirstPageAdd) >> 12));	
	int i = 0; 
	int npage;
	
	
    	for (i = 0; i < currentpage->consec; i++) 
	{
		currentpage[i].allocated=0; 
		currentpage[i].kernel=0; 
		kprintf("i freed %d pages\n",i+1);
	}	
	coremap.numofpageleft=coremap.numofpageleft+currentpage->consec;
	splx(spl);
}
#if 0
int onStack (struct addrspace* as, vaddr_t faultaddress) {
  if ((USERSTACK - as->as_stackpbase) >= 5001216) {
    panic("Panic for now. EFAULT: user is allocating pass 5 megabytes of stack.");

    return 0; // Here, we are limiting the maximum stack size to be 5 megabytes
  }
  if (faultaddress <= USERSTACK && faultaddress >= as->as_stackpbase ) return 1;
  if (faultaddress <= USERSTACK && faultaddress >= (as->as_stackpbase - PAGE_SIZE)) return 1;
  return 0;
}

int onHeap (struct addrspace* as, vaddr_t faultaddress) {
  if (faultaddress >= as->heapstart && faultaddress <= as->heapend) return 1;
  return 0;
}

int increaseStack (struct addrspace* as, vaddr_t faultaddress, paddr_t* _paddr) {
  assert((faultaddress & PAGE_FRAME) == faultaddress);
  assert(as != NULL);

  struct page* freePageFrame = NULL;
  int paddr;
  if (getOnePhysicalFrame(&freePageFrame)) {
    assert(freePageFrame == NULL);
    return -1; // fail
  }
  if (addMapping(as, faultaddress, &freePageFrame)) return -1; // fail

  paddr = coremap.FirstPageAdd + (freePageFrame - coremap.firstPage)*4096;
  //bzero((void*)paddr, PAGE_SIZE);
  as->as_stackpbase -= PAGE_SIZE;
  *_paddr = paddr;
  return 0;
}

int getOnePhysicalFrame (struct page** freePageFrame) {
  assert(*freePageFrame == NULL);
  assert(curspl > 0);
  struct page* pageEntry = coremap.firstPage;
  while (pageEntry < coremap.firstPage + coremap.numofpage) {
    if ((pageEntry < coremap.firstPage + coremap.numofpage) && pageEntry->allocated == 0) {
      pageEntry->allocated = 1;
      *freePageFrame = pageEntry;
      return 0;
    }
    pageEntry++;
  }
  return -1;
}

int addMapping (struct addrspace* as, vaddr_t faultaddress, struct page** freePageFrame) {
  assert((*freePageFrame)->allocated == 1);
  assert((faultaddress & PAGE_FRAME) == faultaddress);
  assert(as != NULL);

  int lvlOnePageTableIndex = (int)faultaddress>>22;
  int lvlTwoPageTableIndex = ((int)faultaddress>>22)&0x3ff;
  struct lvonepagetable* lvlOnePageTable = &(as->pageTable);
  struct lvtwopagetable* lvlTwoPageTable = lvlOnePageTable->lvtwo[lvlOnePageTableIndex];

  if (lvlTwoPageTable == NULL) {
    lvlTwoPageTable = kmalloc(sizeof(struct lvtwopagetable));
    if (lvlTwoPageTable == NULL) return ENOMEM;
    //bzero(lvlTwoPageTable, sizeof(lvlTwoPageTable));
    lvlOnePageTable->lvtwo[lvlTwoPageTableIndex] = lvlTwoPageTable;
  }

  struct pagetableentry* pte = &(lvlTwoPageTable->entry[lvlTwoPageTableIndex]);
  /* Not sure if these signals are correct */
  /* Everything is readable and everything is writable */
  pte->dirty = 1;
  pte->writable = 1;
  /* Not sure if these signals are correct --END-- */
  pte->valid = 1; // I think this should be correct...
  pte->pageframeadd = coremap.FirstPageAdd + (*freePageFrame - coremap.firstPage)*4096;
  pte->pagePtr = *freePageFrame; // Perhaps redundant pointer
  assert((*freePageFrame)->allocated == 1); // Extra assertions to make sure
  (*freePageFrame)->isKernelPage = 0;
  (*freePageFrame)->as = as; // Perhaps another redundant pointer
  (*freePageFrame)->pte = pte;
  (*freePageFrame)->va = faultaddress; // This is stored unshifted
  return 0;
}

int increaseHeap (struct addrspace* as, vaddr_t faultaddress, paddr_t* _paddr) {
  assert((faultaddress & PAGE_FRAME) == faultaddress);
  assert(as != NULL);

  struct page* freePageFrame = NULL;
  int paddr;
  if (getOnePhysicalFrame(&freePageFrame)) {
    assert(freePageFrame == NULL);
    return -1; // fail
  }
  if (addMapping(as, faultaddress, &freePageFrame)) return -1; // fail

  paddr = coremap.FirstPageAdd + (freePageFrame - coremap.firstPage)*4096;
  //bzero((void*)paddr, PAGE_SIZE);
  as->heapend += PAGE_SIZE;
  *_paddr = paddr;
  return 0;
}

int loadPage (struct addrspace* as, vaddr_t faultaddress, paddr_t* _paddr, int region) {
  if (region == 1) assert((faultaddress >= as->as_vbase1) && (faultaddress < as->as_vbase1 + as->as_npages1 * PAGE_SIZE));
  if (region == 2) assert((faultaddress >= as->as_vbase2) && (faultaddress < as->as_vbase2 + as->as_npages2 * PAGE_SIZE));
  assert((faultaddress & PAGE_FRAME) == faultaddress);
  assert(as != NULL);
  assert(as->v != NULL);

  struct page* freePageFrame = NULL;
  int paddr, i;
  if (getOnePhysicalFrame(&freePageFrame)) {
    assert(freePageFrame == NULL);
    kprintf("failed to getOnePhysicalFrame\n");
    return -1; // fail
  }
  if (addMapping(as, faultaddress, &freePageFrame)) {
    kprintf("failed to add mapping\n");
    return -1; // fail
  }

  assert(freePageFrame != NULL);
  assert(freePageFrame->va == faultaddress);
  paddr = coremap.FirstPageAdd + (freePageFrame - coremap.firstPage)*4096;
  //bzero((void*)paddr, PAGE_SIZE); This line causes kernel crash. Perhaps a bug? Have time, I'll come back.

  /* start */
  u_int32_t p_vaddr1 = as->progheader[0].p_vaddr;
  u_int32_t p_vaddr2 = as->progheader[1].p_vaddr;
  //struct vnode* v = as->v;
  size_t offset1 = as->offset1;
  size_t offset2 = as->offset2;
  int memSize1 = as->memSize1;
  int memSize2 = as->memSize2;
  int fileSize1 = as->fileSize1;
  int fileSize2 = as->fileSize2;
  int base1 = as->as_vbase1;
  int base2 = as->as_vbase2;

  int result = -1;
  if (faultaddress >= p_vaddr1 && faultaddress < p_vaddr1 + memSize1) {
    //panic("out1\n");
    assert(region == 1);
    result = load(as, faultaddress, base1, paddr, p_vaddr1, offset1, memSize1, fileSize1);
  } else if (faultaddress >= p_vaddr2 && faultaddress < p_vaddr2 + memSize2) {
    //panic("out2\n");
    assert(region == 2);
    result = load(as, faultaddress, base2, paddr, p_vaddr2, offset2, memSize2, fileSize2);
  } else {
    panic("I shouldn't be here.");
  }
  if (result) return result;
  /* start --END-- */

  *_paddr = paddr;
  return 0;
}

int load (struct addrspace* as, vaddr_t faultaddress, u_int32_t base, paddr_t paddr, u_int32_t vaddr, off_t offset, int memSize, int fileSize) {
  assert((faultaddress&PAGE_FRAME) == faultaddress);
  struct uio u;
  int result;
  size_t fillamt;
  struct vnode* v = as->v;

  int pageOffset = (faultaddress - base)/PAGE_SIZE;
  assert(pageOffset >= 0);

  if (fileSize > PAGE_SIZE) fileSize = PAGE_SIZE;
  if (fileSize < 0) fileSize = 0;

  if (fileSize > memSize) {
		kprintf("ELF: warning: segment filesize > segment memsize\n");
		fileSize = memSize;
  }

	u.uio_iovec.iov_kbase = paddr;
	u.uio_iovec.iov_len = memSize;
	u.uio_resid = fileSize;
	u.uio_offset = offset + pageOffset*PAGE_SIZE;
	u.uio_segflg = UIO_SYSSPACE;
	u.uio_rw = UIO_READ;
	u.uio_space = NULL;

	result = VOP_READ(v, &u);
	if (result) {
		return result;
	}

	if (u.uio_resid != 0) {
		/* short read; problem with executable? */
		kprintf("ELF: short read on segment - file truncated?\n");
		return ENOEXEC;
	}

  /* I don't think we need this anymore */
	fillamt = memSize - fileSize;
	if (fillamt > 0) {
		DEBUG(DB_EXEC, "ELF: Zero-filling %lu more bytes\n", 
		      (unsigned long) fillamt);
		u.uio_resid += fillamt;
		result = uiomovezeros(fillamt, &u);
	}
  /* I don't think we need this anymore --END-- */
	
	return result;

}
#endif

/*
 * Right now, vm_fault is using spl. We night need to add in a lock when swapping.
 */
int
vm_fault(int faulttype, vaddr_t faultaddress)
{
<<<<<<< .mine
    kprintf("This is the fault address wtf: %x\n",faultaddress);
=======
	//kprintf("fault addres is %x\n",faultaddress);
>>>>>>> .r24
	vaddr_t vbase1, vtop1, vbase2, vtop2, stackbase, stacktop;
	paddr_t paddr;
	int i;
	u_int32_t ehi, elo;
	struct addrspace *as;
	int spl;

	spl = splhigh();

	faultaddress &= PAGE_FRAME;

	DEBUG(DB_VM, "dumbvm: fault: 0x%x\n", faultaddress);

	switch (faulttype) {
	    case VM_FAULT_READONLY:
		/* We always create pages read-write, so we can't get this */
		panic("dumbvm: got VM_FAULT_READONLY\n");
	    case VM_FAULT_READ:
	    case VM_FAULT_WRITE:
		break;
	    default:
		splx(spl);
		return EINVAL;
	}

	as = curthread->t_vmspace;
	if (as == NULL) {
		/*
		 * No address space set up. This is probably a kernel
		 * fault early in boot. Return EFAULT so as to panic
		 * instead of getting into an infinite faulting loop.
		 */
		return EFAULT;
	}
#if 0
  int result = -1;
  if (onStack(as, faultaddress)) {
    kprintf("updating stack\n");
    result = increaseStack(as, faultaddress, &paddr);
  } else if (onHeap(as, faultaddress)) {
    kprintf("updating heap\n");
    result = increaseHeap(as, faultaddress, &paddr);
  } else {
    kprintf("updating data and text\n");
    if ((faultaddress >= as->as_vbase1) && (faultaddress < (as->as_vbase1 + as->as_npages1 * PAGE_SIZE))) {
      result = loadPage(as, faultaddress, &paddr, 1);
    } else if ((faultaddress >= as->as_vbase2) && (faultaddress < (as->as_vbase2 + as->as_npages2 * PAGE_SIZE))) {
      result = loadPage(as, faultaddress, &paddr, 2);
    } else {
      kprintf("This is as->as_vbase1: %d\n",as->as_vbase1);
      kprintf("This is as->as_vbase2: %d\n",as->as_vbase2);
      kprintf("This is the fault address: %d\n",faultaddress);
    }
  }

  if (result) {
    splx(spl);
    kprintf("START:SLKDFJSD:FJKSDL:JFK:SDJFK:LSDJFK:JSK:DLFJ:SDFJK:SDFJK:SDFJK:SDF\n");
    kprintf("This is USERSTACK - as->heapstart: %x\n",USERSTACK-as->heapstart);
    kprintf("This is USERSTACK - as->as_stackpbase: %x\n",USERSTACK - as->as_stackpbase);
    kprintf("This is as->heapend - as->heapstart: %x\n",as->heapend-as->heapstart);
    kprintf("This is as->as_vbase1: %x\n",as->as_vbase1);
    kprintf("This is as->as_vbase2: %x\n",as->as_vbase2);
    kprintf("This is the fault address: %x\n",faultaddress);
    kprintf("This is as->as_vbase1 + as->as_npages1 * PAGE_SIZE: %x\n",as->as_vbase1 + as->as_npages1 * PAGE_SIZE);
    kprintf("This is as->as_vbase2 + as->as_npages2 * PAGE_SIZE: %x\n",as->as_vbase2 + as->as_npages2 * PAGE_SIZE);
    kprintf("START:SLKDFJSD:FJKSDL:JFK:SDJFK:LSDJFK:JSK:DLFJ:SDFJK:SDFJK:SDFJK:SDF\n");

    return EFAULT;
  }
#endif

	/* Assert that the address space has been set up properly. */
	assert(as->as_vbase1 != 0);
	assert(as->as_pbase1 != 0);
	assert(as->as_npages1 != 0);
	assert(as->as_vbase2 != 0);
	assert(as->as_pbase2 != 0);
	assert(as->as_npages2 != 0);
	assert(as->as_stackpbase != 0);
	assert((as->as_pbase1 & PAGE_FRAME) == as->as_pbase1);
	assert((as->as_vbase2 & PAGE_FRAME) == as->as_vbase2);
	assert((as->as_pbase2 & PAGE_FRAME) == as->as_pbase2);
	assert((as->as_stackpbase & PAGE_FRAME) == as->as_stackpbase);

	vbase1 = as->as_vbase1;
	vtop1 = vbase1 + as->as_npages1 * PAGE_SIZE;
	vbase2 = as->as_vbase2;
	vtop2 = vbase2 + as->as_npages2 * PAGE_SIZE;
	stackbase = USERSTACK - DUMBVM_STACKPAGES * PAGE_SIZE;
	stacktop = USERSTACK;

	if (faultaddress >= vbase1 && faultaddress < vtop1) {
		paddr = (faultaddress - vbase1) + as->as_pbase1;
	}
	else if (faultaddress >= vbase2 && faultaddress < vtop2) {
		paddr = (faultaddress - vbase2) + as->as_pbase2;
	}
	else if (faultaddress >= stackbase && faultaddress < stacktop) {
		paddr = (faultaddress - stackbase) + as->as_stackpbase;
	}
	else {
		splx(spl);
		return EFAULT;
	}

	/* make sure it's page-aligned */
	assert((paddr & PAGE_FRAME)==paddr);

	for (i=0; i<NUM_TLB; i++) {
		TLB_Read(&ehi, &elo, i);
		if (elo & TLBLO_VALID) {
			continue;
		}
		ehi = faultaddress;
		elo = paddr | TLBLO_DIRTY | TLBLO_VALID;
		DEBUG(DB_VM, "dumbvm: 0x%x -> 0x%x\n", faultaddress, paddr);
		TLB_Write(ehi, elo, i);
		splx(spl);
    kprintf("Got it done\n");
		return 0;
	}

	kprintf("dumbvm: Ran out of TLB entries - cannot handle page fault\n");
	splx(spl);
	return EFAULT;
}
<<<<<<< .mine
=======


static void swapper(void)
{
int numfree;
int spl;
while (1)
	{
	lock_acquire(&swap_lock);
		spl = splhigh();
        	numfree = coremap.numofpageleft;
        	splx(spl);
	while(numfree>0)
		{
		cv_broadcast(&cv_needmore, &swap_lock);
		cv_wait(&cv_needevict, &swap_lock);
			spl = splhigh();
        		numfree = coremap.numofpageleft;
        		splx(spl);
		}

		spl=splhigh();
		
		splx(spl);
	lock_release(&swap_lock);	
	}
}

>>>>>>> .r24
